{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7126e524-8b2f-42e0-b505-0be7b1df9119",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ac3364f9-c59e-4036-855b-2550b4ec9d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded Liar.csv to /home/ec2-user/SageMaker/data/Liar.csv\n",
      "✅ Downloaded Synthetic Financial Datasets.csv to /home/ec2-user/SageMaker/data/Synthetic Financial Datasets.csv\n",
      "✅ Downloaded WELFake_Dataset.csv to /home/ec2-user/SageMaker/data/WELFake_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Define S3 bucket and file names\n",
    "s3_bucket = \"fake-news-raw-data\"\n",
    "s3_files = [\"Liar.csv\", \"Synthetic Financial Datasets.csv\", \"WELFake_Dataset.csv\"]\n",
    "local_folder = \"/home/ec2-user/SageMaker/data/\"\n",
    "\n",
    "# Ensure local directory exists\n",
    "os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Download files from S3\n",
    "for file in s3_files:\n",
    "    local_path = os.path.join(local_folder, file)\n",
    "    s3_client.download_file(s3_bucket, file, local_path)\n",
    "    print(f\"✅ Downloaded {file} to {local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88449319-68f1-4406-b075-e2ba10e9c945",
   "metadata": {},
   "source": [
    "## Import Python package \"stopwords\" to overlook commonly used words and articles (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ee5818f-9ebe-4c53-a279-1fc322eac88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18ec09-accd-49bf-9fd6-d58270b01c07",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a6026d4a-a63e-404c-ae35-f85af2c408aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbbea4-b9e7-4731-91d1-40ba700c3e1f",
   "metadata": {},
   "source": [
    "## Ensure NLTK stopwords are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a8a6ba6c-f9de-4124-b7e2-7d9890cc7031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5dbcb-e86f-44b4-a6d8-7b5bae37355e",
   "metadata": {},
   "source": [
    "## Load Liar.csv dataset and make pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "270e03e4-3643-4374-a2d2-ff61cfc9d7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10240 entries, 0 to 10239\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Statement ID          10240 non-null  object \n",
      " 1   Lie_label             10240 non-null  object \n",
      " 2   Statement             10240 non-null  object \n",
      " 3   Topic                 10238 non-null  object \n",
      " 4   Speaker               10238 non-null  object \n",
      " 5   Speaker_Job_Title     7342 non-null   object \n",
      " 6   State                 8030 non-null   object \n",
      " 7   Speaker_party         10238 non-null  object \n",
      " 8   barely_true_counts    10238 non-null  float64\n",
      " 9   false_counts          10238 non-null  float64\n",
      " 10  half-true_counts      10238 non-null  float64\n",
      " 11  mostly_true_counts    10238 non-null  float64\n",
      " 12  pants_on_fire_counts  10238 non-null  float64\n",
      " 13  statement_mode        10138 non-null  object \n",
      "dtypes: float64(5), object(9)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "liar_clean = pd.read_csv(\"/home/ec2-user/SageMaker/data/Liar.csv\")\n",
    "\n",
    "# Display dataset info\n",
    "\n",
    "liar_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062bd7a-8261-45a4-a21d-0e61a02d2d90",
   "metadata": {},
   "source": [
    "## Function to clean text using NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "762a82f1-f45f-4279-a0b9-fb72f0663648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_nltk(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "liar_clean[\"clean_statement\"] = liar_clean[\"Statement\"].apply(clean_text_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ec486-02bc-4d11-ade3-0ce948994fd2",
   "metadata": {},
   "source": [
    "## Convert numerical columns to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5aa46f8b-66f2-4b4a-853b-f0aa3a3b9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cols = [\"barely_true_counts\", \"false_counts\", \"half-true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\"]\n",
    "liar_clean[count_cols] = liar_clean[count_cols].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af357075-ef5c-418b-a583-51ac7b22c3e8",
   "metadata": {},
   "source": [
    "## Create a total misinformation score feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f7ec1bd6-763b-49a9-a386-2d9d4d4f601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_clean[\"total_misinfo_score\"] = liar_clean[count_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e17a4-e42b-4762-814e-1d13128cc782",
   "metadata": {},
   "source": [
    "## One-Hot Encoding for Speaker Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7fdef977-36b6-49e1-bf6a-fd5246659a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False,drop=\"first\")\n",
    "encoded_party = encoder.fit_transform(liar_clean[[\"Speaker_party\"]])\n",
    "party_columns = encoder.get_feature_names_out([\"Speaker_party\"])\n",
    "liar_encoded_party = pd.DataFrame(encoded_party, columns=party_columns, index=liar_clean.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d04bbe-b158-487b-8a82-ce3d62bb09b5",
   "metadata": {},
   "source": [
    "## Merge and drop original categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "45262a02-0605-4a7d-8317-90b789be35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_clean = pd.concat([liar_clean, liar_encoded_party], axis=1)\n",
    "liar_clean.drop([\"Speaker_party\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b9e18-3d78-4f77-a07c-0e98c1898abc",
   "metadata": {},
   "source": [
    "## Balance dataset by oversampling minority classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9c177407-11ec-4319-a8e4-9109d8596a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class = liar_clean[liar_clean[\"Lie_label\"] == \"FALSE\"]\n",
    "minority_classes = liar_clean[liar_clean[\"Lie_label\"] != \"FALSE\"]\n",
    "minority_classes_upsampled = resample(minority_classes, replace=True, n_samples=len(majority_class), random_state=42)\n",
    "liar_balanced = pd.concat([majority_class, minority_classes_upsampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a00a7b-3591-44ee-96ab-a9b4d72683c5",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6dc8145b-51ba-4d08-bfc8-55215ae37243",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(liar_balanced, test_size=0.2, random_state=42, stratify=liar_balanced[\"Lie_label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f000a-9843-413a-bf9a-f8fc087d21b0",
   "metadata": {},
   "source": [
    "## Save the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e98c9559-4997-4db7-bdbf-2d8d601296a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing on Liar dataset complete. Training and test datasets saved.\n"
     ]
    }
   ],
   "source": [
    "train_data.to_csv(\"/home/ec2-user/SageMaker/data/Liar_train.csv\", index=False)\n",
    "test_data.to_csv(\"/home/ec2-user/SageMaker/data/Liar_test.csv\", index=False)\n",
    "\n",
    "print(\"Preprocessing on Liar dataset complete. Training and test datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac07402-cf14-4f75-bae9-4450980b71b4",
   "metadata": {},
   "source": [
    "**References**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b6742-35af-442e-a054-d30f4fe26aa3",
   "metadata": {},
   "source": [
    "OpenAI. (2025). ChatGPT (March 20 version). [LLM]. https://chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffc540-5cfd-4006-807c-dca72415b425",
   "metadata": {},
   "source": [
    "Python Tutorials. (2021, July 22). *NLTK stop words.* pythonspot. Accessed March 20, 2025 from https://pythonspot.com/nltk-stop-words/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
